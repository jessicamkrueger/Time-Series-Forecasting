---
title: "Monthly Forecast"
output:
  html_document: default
  pdf_document: default
---
Load all necessary libraries and data sets.

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(gridExtra)
library(vars)
library(forecast)
library(fpp2)
library(lubridate)
library(scales)
library(lemon)
options(scipen=999)
library(GGally)
library(plotly)
library(seasonal)
library(data.table)

setwd("~/Ubiqum/Project 4/Task 1/Original Data Set")
power2 <- readRDS("~/Ubiqum/Project 4/Task 1/Original Data Set/power2.rds")
```

I start by creating a data frame containing montly totals in watt-hours for each sub meter as well as a total for the entire household. I then store this as a time-series object.

```{r}
pMonth <- power2 %>%
  group_by(month=floor_date(DateTime, "month")) %>%
  summarize_at(vars(S1_Kitchen:S4_Rest), sum, na.rm = TRUE) %>%
  mutate(total = S1_Kitchen + S2_Laundry + S3_WH_AC + S4_Rest)

pMonthTS <- ts(pMonth, frequency = 12, start = c(2006, 12))
```

We can now plot the time series to see that there is seasonality present in the monthly household consumption totals. We can also make a seasonal plot showing the seasonal variation by year.

```{r}
autoplot(pMonthTS[,"total"]) +
  ggtitle("Total Household Electric Consumption 2006-2010") +
  xlab("Year") +
  ylab("Watt Hours")

ggseasonplot(pMonthTS[,"total"], year.labels=TRUE) +
  ylab("Watt Hours") +
  ggtitle("Seasonal Plot: Total Household Consumption")
```

Next, I create a lag plot as well as an autocorrelation plot to further explore seasonality in the data set.

```{r}
gglagplot(pMonthTS[,6])

ggAcf(pMonthTS[,6]) 
```

Using the tsclean function, we identify and replace any outliers (with suggested values) and missing values so that our model performs better.
```{r}
tsoutliers(pMonthTS[,6])
pMonthClean <- tsclean(pMonthTS[,6])
```

Now, we partition the data into a training and testing set. I want to use a full year as a testing set in order to ensure my forecast for the next 12 months is useful.

Next, I train a linear model on the testing set using trend and seasonality as predictors. I also plot the model against the original data.

```{r}
train1 <- window(pMonthClean,start=c(2006,12), end=c(2009,11))
test1 <- window(pMonthClean, start=c(2009,11))

LMMonth <- tslm(train1 ~ trend + season)
summary(LMMonth)

autoplot(train1, series="Data") +
  autolayer(fitted(LMMonth), series="Fitted") +
  xlab("Year") + 
  ylab("Watt Hours") +
  ggtitle("Montly Electric Consumption")


```

I also check the residuals and plot the errors of the model.
```{r message=FALSE, warning=FALSE}
accuracy(LMMonth)
checkresiduals(LMMonth)

cbind(Data=pMonthClean, Fitted=fitted(LMMonth)) %>%
  as.data.frame() %>%
  ggplot(aes(x = Data, y = Fitted)) +
  geom_point() +
  ylab("Fitted") + xlab("Actual values") +
  ggtitle("Monthly Consumption - Linear Model Errors") +
  geom_abline(intercept=0, slope=1)
```

Next, I apply the model to forecast testing set and plot the forecast against the acutal values from our testing set. I then examine the performance metrics of the testing and training sets as well as the plot to determine the model's accuracy.

The auto-correlation plot of residuals has a significant peak at lag 8 meaning that there is still some useful information in the data that is not captured by my model.

```{r}
LMTestfcast <- forecast(LMMonth, h=12)

autoplot(LMTestfcast, series="Forecast") +
  autolayer(test1, series="Observed") +
  ggtitle("Monthly Consumption Forecast on Testing Data") +
  xlab("Year") + ylab("Watt Hours")

```


Finally, I deploy the model on the entire data set to forecast the next 12 months of household energy consumption.
```{r}
LMMonthfinal <- tslm(pMonthClean ~ trend + season)
LMFinalFcast <- forecast(LMMonthfinal, h=12)

autoplot(LMFinalFcast)+
  autolayer(fitted(LMMonthfinal), series="Model") +
  ggtitle("Monthly Consumption Forecast for Next 12 Months") +
  xlab("Year") + ylab("Watt Hours")

summary(LMMonthfinal)
accuracy(LMMonthfinal)
checkresiduals(LMMonthfinal)

summary(LMFinalFcast)
```

Now that we have forecasted the next 12 months of energy consumption, and we know that there is trend and seasonality to the data, we can decompose the time-series data to separate out the season and the trend. This plot shows the seasonal component, the trend component, and the remainder component of the time series data set.

We see from the decomposed plot that there is siginficant seasonality, but we also see a decreasing trend which is not as easily visible from the full data set.

```{r}
pMonthD <- decompose(pMonthClean)

pMonthClean %>% decompose(type="multiplicative") %>%
  autoplot() + xlab("Year") +
  ggtitle("Decomposed Monthly Electric Consumption in Watt-Hours")
```

This next section shows a seasonally-adjusted plot of the time series data.
```{r}
pMonth.stl <- stl(pMonthClean,"periodic")
pMonth.sa <- seasadj(pMonth.stl)  

autoplot(pMonthClean) +
  ggtitle("Original Data")
autoplot(pMonth.sa) +
  ggtitle("Seasonally Adjusted Data")
```



Next, I'll use the Simple Exponential Smoothing (SES) model to create a model to predict monthly consumption. The prediction intervals show that there is considerable uncertainty in the future values over the next 12 months of the forecast period.

```{r}
sesMonth <- ses(pMonthClean, h = 12)

round(accuracy(sesMonth),2)
checkresiduals(sesMonth)

autoplot(sesMonth) +
  autolayer(fitted(sesMonth), series="Fitted") +
  xlab("Year") +
  ylab("Watt-Hours of Electric Consumption")
```
Now, we'll try the Holt Winters model using both additive and multiplicative seasonal parameters. From the summary statistics, we see that the multiplicative method performs best.

alpha = weight give to the level component of the model
beta = weight given to the trend component of the model
gamme = weight given to the seasonal component of the model

```{r}
fit1 <- hw(pMonthClean,seasonal="additive", alpha = .2, gamma = .01)
fit2 <- hw(pMonthClean,seasonal="multiplicative")
autoplot(pMonthClean) +
  autolayer(fit1, series="HW additive forecasts", PI=FALSE) +
  autolayer(fit2, series="HW multiplicative forecasts",
    PI=FALSE) +
  xlab("Year") +
  ylab("Watt-Hours of Electric Consumption") +
  guides(colour=guide_legend(title="Forecast"))

autoplot(pMonthClean) +
  autolayer(fitted(fit1), series = "HW additive forecasts") +
  autolayer(fitted(fit2), series = "HW multiplicative forecasts") +
  ggtitle("Fitted Values vs Observed Values")

checkresiduals(fit1)
summary(fit1)

checkresiduals(fit2)
summary(fit2)
```
In this section, I try a holt-winters multiplicative model with a damped trend.
```{r}
fit2.5 <- hw(pMonthClean,damped = TRUE, seasonal="multiplicative")
autoplot(pMonthClean) +
  autolayer(fit2.5, series="HW mult. damped forecasts", PI=FALSE) +
  autolayer(fit2, series="HW multiplicative forecasts",
    PI=FALSE) +
  xlab("Year") +
  ylab("Watt-Hours of Electric Consumption") +
  guides(colour=guide_legend(title="Forecast"))

summary(fit2.5)
checkresiduals(fit2.5)

```

Next, I use hte ets() function to select the best model.

```{r}
fit3 <- ets(pMonthClean)
summary(fit3)
autoplot(fit3)
checkresiduals(fit3)

etS <- forecast(fit3, h=12)

fit3 %>% forecast(h=12) %>%
  autoplot() +
  ylab("Electric Consumption in Watt-Hours")
```

In the next section, I use the auto.arima function to select an appropriate ARIMA model.
 
```{r}
fit4 <- auto.arima(pMonthClean)
checkresiduals(fit4)
summary(fit4)

arimA <-forecast(fit4, h=12)

autoplot(forecast(fit4, h=12))

```

The linear model using trend and seasonality performed the best of all the models tested. Using the results of that model, we can predict the consumption and the electric bill cost for the next 12 months.

Below are two different visualizations of the predicted electric bill for the next 12 months.

```{r message=FALSE, warning=FALSE}

MonthFC <- as.data.frame(summary(LMFinalFcast))
MonthFC <- setDT(MonthFC, keep.rownames = TRUE)[]

#Adding cost column using .147 EUR/kWh as standard cost
MonthCosts <- MonthFC %>%
  mutate(`Point Forecast`= `Point Forecast`/1000) %>%
  mutate(`Lo 80`= `Lo 80`/1000) %>% 
  mutate(`Hi 80`= `Hi 80`/1000) %>%
  mutate(`Lo 95`= `Lo 95`/1000) %>%
  mutate(`Hi 95`= `Hi 95`/1000) %>%
  mutate_at(vars(`Point Forecast`:`Hi 95`), (funs(cost = . *.147))) %>%
  gather(Forecast, kWh, `Point Forecast`:`Hi 95`)  %>%
  select(rn, Forecast, kWh) %>%
  mutate(Cost = kWh*.147)
  
  
#plot predictions for monthly bill (only showing 95PI)
MonthCosts$Month <- parse_date_time(MonthCosts$rn, "my") 
MonthCosts95 <- filter(MonthCosts, Forecast != "Lo 80" & Forecast != "Hi 80")

ggplot(MonthCosts95, aes(x=Month, y=Cost, color=Forecast, group=Forecast)) +
  geom_line() +
  scale_y_continuous(labels = dollar, breaks = seq(40, 180, 10)) +
  labs(x="Month", y="Predicted Electric Bill", 
       title = "Household Electric Bill: 12 Month Forecast")

#using a pointrange plot
MonthPR <- MonthFC %>%
  mutate_at(vars(`Point Forecast`,`Hi 95`, `Lo 95`), 
            (funs(cost = round((. /1000)*.147)))) %>%
  dplyr::rename(Point_Forecast = `Point Forecast_cost`, 
                Forecast_Min = `Lo 95_cost`,
                Forecast_Max = `Hi 95_cost`,
                Month = rn)

MonthPR$Month <- parse_date_time(MonthPR$Month, "my")        
#add column that shows is point forecast is above or below average
MonthPR$Relative_Cost <- (MonthPR$Point_Forecast < mean(MonthPR$Point_Forecast))

ggplot(MonthPR, aes(x=Month, y=Point_Forecast)) +
  geom_pointrange(aes(ymin=Forecast_Min, ymax=Forecast_Max, 
                      color = Relative_Cost), size = 1) +
  geom_text(aes(label = Point_Forecast), nudge_y = 30) +
  scale_y_continuous(labels = dollar, breaks = seq(40, 180, 10)) +
  labs(x="Month", y="Predicted Bill Cost", 
       title = "Household Electric Bill: 12 Month Forecast",
       subtitle = "With 95% Prediction Interval Range") +
  scale_x_datetime(date_labels="%b %y", date_breaks ="1 month") +
  scale_color_discrete("Bill Type", 
                      labels=c("Above Average", "Below Average"))

```

